# -*- coding: utf-8 -*-
"""Consumer Study based on Automobile Sales Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ta_iL6bo-9FYzD3Gh-F0s0CyS1zNX3SA

# Consumer Study based on Automobile Sales Data

# Project Plan

Selecting the dataset for a project where all data analysis and data science skills need to be put to use is the most intregal part of the project. Hence, The dataset is sourced from Kaggle (https://www.kaggle.com/datasets/ddosad/auto-sales-data/data) [1]. It encompasses information on customer orders, individual prices of items sold in bulk orders, and other relevant details and presents a variety of information that provides crucial insights into the sales trends, purchasing and behavior, offering a valuable resource for businesses working to understand and deliver to their customer base more effectively. This dataset, contains a total of 2747 records, each contributing to a deep exploration of customer preferences and purchase trends.

A deeper understanding of the attributes of the dataset are as follows [1]:

* ORDERNUMBER: This column represents the unique identification number assigned to each order.
* QUANTITYORDERED: It indicates the number of items ordered in each order.
* PRICEEACH: This column specifies the price of each item in the order.
* ORDERLINENUMBER: It represents the line number of each item within an order.
* SALES: This column denotes the total sales amount for each order, which is calculated by multiplying the quantity ordered by the price of each item.
* ORDERDATE: It denotes the date on which the order was placed.
* DAYS_SINCE_LASTORDER:	This column represents the number of days that have passed since the last order for each customer. It can be used to analyze customer purchasing patterns.
* STATUS: It indicates the status of the order, such as "Shipped," "In Process," "Cancelled," "Disputed," "On Hold," or "Resolved."
* PRODUCTLINE: This column specifies the product line categories to which each item belongs.
* MSRP:	It stands for Manufacturer's Suggested Retail Price and represents the suggested selling price for each item.
* PRODUCTCODE: This column represents the unique code assigned to each product.
* CUSTOMERNAME: It denotes the name of the customer who placed the order.
* PHONE: This column contains the contact phone number for the customer.
* ADDRESSLINE1:	It represents the first line of the customer's address.
* CITY:	This column specifies the city where the customer is located.
* POSTALCODE: It denotes the postal code or ZIP code associated with the customer's address.
* COUNTRY: This column indicates the country where the customer is located.
* CONTACTLASTNAME: It represents the last name of the contact person associated with the customer.
* CONTACTFIRSTNAME: This column denotes the first name of the contact person associated with the customer.
* DEALSIZE:	It indicates the size of the deal or order, which are the categories "Small," "Medium," or "Large."

Altogether, sales datasets are a very important part in the world of business as they help in understanding purchasing behaviour at a deeper level along with contributing towards building marketing strategies that increase profitability, tailoring product offers, enhance customer satisfaction and to build a monopoly in the market of retailing.

## Project Aim and Objectives (5 marks)

The primary aim of this project is to conduct a comprehensive analysis of the Automobile Sales Dataset, using various data analysis and machine learning techniques. By looking into into the details and attributes of customer details and purchasing patterns, the project seeks to extract actionable insights that businesses can employ to optimize their products, marketing strategies, and overall profitability.

The project will begin with exploratory data analysis (EDA) to gain a deep understanding of the dataset's structure and characteristics. This involves statistical summaries, visualizations, and the identification of patterns and trends within the data. Subsequently, data cleaning and preprocessing steps will be implemented to ensure the dataset's integrity and prepare it for analysis.

Clustering algorithms will be employed to identify distinct customer segments based on different criteria such as an RFM analysis, allowing for targeted marketing strategies tailored to specific groups of customers.

Following the Clustering of data, Machine learning models will be implemented such that the data is trained and tested to provide prediction for a target variable, helping the business to be prepared according to the proposed predictions. Along with modelling, accuracy of the models will be tested and the best among them will be chosen.

Furthermore, the project aims to generate insightful visualizations, such as interactive charts and graphical representations, to communicate the findings effectively. Visualizations will be created using popular Python libraries like Matplotlib, and Seaborn, providing a compelling and accessible means to present complex patterns and trends within the dataset.

To sum up, the objective of this project is to not only study the Automobile Sales Dataset but also to offer practical suggestions to companies trying to match their marketing plans with market demands. The projectâ€™s goal is to give organizations the tools they need to stay competitive in a changing market, improve customer happiness, and make informed decisions by combining machine learning modeling, customer segmentation, exploratory data analysis, and impactful visualizations.

### Specific Objectives

**Project Objectives:**

1. **Exploratory Analysis of Purchase Patterns:**
   -  Conduct in-depth exploratory data analysis to uncover patterns, relationships between vairables and trends in customer purchase behavior, considering factors such as deal size, sales, product line, and geographic variables.
   
2. **Customer Segmentation and Profiling [2]:**
   -  Employ clustering algorithms to identify distinct customer segments based on recency, frequency and monetary value.

3. **Predictive Modeling:**
   -  Develop a predictive model to determine the price of each order using variables such as MSRP.
   
4. **Visualization of Predictive models:**
   -  Create visualizations to represent the accuracy of different models by comparing true values vs predicted values and therefore choosing the best model.


## System Design


### Architecture

1) Data Collection:
Raw data is collected from various sources, such as customer transactions, preferences, and interactions.

2) Exploratory Data Analysis (EDA):
The raw data undergoes EDA, where statistical analyses, visualizations, and pattern identification take place. This phase provides a deep understanding of the dataset.

3) Data Cleaning and Preprocessing:
The dataset is cleaned and preprocessed to ensure data integrity and to prepare it for training different models on it.

4) Machine Learning Modeling:
Utilizing machine learning models, relationships between customer attributes and purchasing decisions are explored. This involves classification models for predicting various customer behaviors and clustering algorithms to identify distinct customer segments.

5) Visualization:
The project generates insightful visualizations using Python libraries like Matplotlib, Seaborn, and Plotly. These visualizations include interactive dashboards and graphical representations, making it easier for stakeholders to comprehend complex patterns and trends.

![image.png](attachment:0f113e2c-7dbf-4f2f-95cd-63ad8eae2cf0.png)

### Processing Modules and Algorithms


1. **Exploratory Data Analysis (EDA):**
   - **Objective:** Gain insights into the dataset and uncover patterns, relationships, and trends in customer purchase behavior.
   - **Algorithm/Technique:** Use descriptive statistics, data visualization (such as histograms, scatter plots, and heatmaps), and correlation analysis to explore the distribution of variables, identify patterns, and understand the underlying structure of the data. Additionally, to perform outlier detection and handling as part of the exploratory process to ensure the data's integrity for subsequent analysis.

2. **Customer Segmentation using Clustering [2]:**
   - **Objective:** Identify distinct customer segments based on recency, frequency, and monetary value (RFM) [2].
   - **Algorithm/Technique:** Utilize clustering algorithms such as K-means or hierarchical clustering to group customers with similar purchasing behavior. This involves transforming the data into feature vectors representing RFM values.

3. **Predictive Modeling for Price Determination:**
   - **Objective:** Develop a model to predict the price of each order based on relevant variables such as MSRP and RFM score.
   - **Algorithm/Technique:** Employ regression algorithms such as linear regression or more advanced methods like random forests or bayesian ridge. This involves training the model on historical data with known prices and then applying it to predict prices for test data.

4. **Visualization of Model Accuracy:**
   - **Objective:** Evaluate and compare the accuracy of different predictive models.
   - **Algorithm/Technique:** Utilize visualizations such as scatter plots or line charts to compare true values against predicted values from different models. This involves implementing metrics like Mean Squared Error (MSE) or R-squared to quantify model performance.

# Program Code

## Importing necessary libraries and data
"""

import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge, BayesianRidge
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Suppressing warnings
warnings.filterwarnings('ignore')

"""---

## Importing the data by creating a dataframe of it
"""

df = pd.read_csv('Auto Sales data.csv', parse_dates = ['ORDERDATE'], dayfirst = True)

"""---

## Getting basic information on the data such as top 5 rows, shape and information about the variables
"""

display(df.head())
print('\n\nShape of the dataset: ',df.shape, '\n\n')
print(df.info())

"""The output presents the initial rows of the DataFrame 'df' to offer a glimpse of its structure. Additionally, it provides essential details about the dataset, including its shape (number of rows and columns) and a summary of data types, aiding in a quick overview of the dataset's characteristics.


---

## Checking for duplicate values in the 'ORDEREDNUMBER' column
"""

print(df['ORDERNUMBER'].duplicated().any(), '\n\n')

display(df[df['ORDERNUMBER'] == 10107].sort_values(by = 'ORDERLINENUMBER'))
print('\n\n')
print(df.info(), '\n\n')
display(round(df.describe(),2).T)

"""In many business scenarios, an order may consist of multiple items, each with its own quantity, order line number, and price. Therefore, it's common to see duplicate order numbers in a dataset where each row corresponds to a unique item within an order.

For example, consider the following hypothetical scenario:

Order 10107 has two items:

Item 1: Quantity 3, Order Line Number 1, Price $20

Item 2: Quantity 2, Order Line Number 2, Price $15

In this case, both rows would have the same order number (10107) and order date but differ in quantity, order line number, and price.



---

## This code adds a 'DAYS_SINCE_LASTORDER' feature to the DataFrame 'df,' representing the days between each order's 'ORDERDATE' and June 1, 2020

We already had a 'DAYS_SINCE_LASTORDER' column in the dataset present but failed to validate the values in that row when compared with the date of last order of each customer. Hence, to avoid data integrity and validation issue we decided to recalculate the entire column based on the logic explained later.
"""

#Creating new feature 'DAYS_SINCE_LASTORDER' based on the time difference:
df = df.drop('DAYS_SINCE_LASTORDER', axis = 1)
df['DAYS_SINCE_LASTORDER'] = (datetime(2020, 6, 1)- df['ORDERDATE']).dt.days
df.head()

"""---

## This code snippet generates descriptive statistics for object-type columns in the DataFrame 'df' using the describe() method, providing insights into categorical data characteristics.
"""

df.select_dtypes(include = ['object']).describe().T

"""---

# Exploratory data analysis (EDA)

## This code visualizes sales distribution through a histogram and product line distribution via a countplot in the DataFrame 'df.'
"""

#Distribution of Sales:
plt.figure(figsize = (8,8))
sns.histplot(df['SALES'], kde = True, color = 'red', stat = 'density')
plt.title('Distribution of Sales - Histogram')
plt.xlabel('Sales')
plt.ylabel('Density')
plt.show()
print('\n\n')

#Distribution of Product line:
plt.figure(figsize = (10,8))
sns.countplot(y = 'PRODUCTLINE', data = df, palette = 'colorblind', order = df['PRODUCTLINE'].value_counts().index)
plt.title('Distribution of Average Sales by Product Line - Bar Plot')
plt.xlabel('Total Units Sold')
plt.ylabel('Product Line')
plt.show()
print('\n\n')

"""## This code visually represents the distribution of countries using a pie chart and the distribution of deal sizes through a count plot in the DataFrame 'df.'"""

#Distribution of Country:
plt.figure(figsize = (10,8))
country_counts = df['COUNTRY'].value_counts()
plt.pie(country_counts, labels = country_counts.index, autopct = '%1.1f%%', colors = sns.color_palette('colorblind'))
plt.title('Distribution of Country - Pie Chart')
plt.show()
print('\n\n')

#Distribution of Deal Size:
plt.figure(figsize = (8,8))
sns.countplot(x = 'DEALSIZE', data = df, palette = 'colorblind', order = df['DEALSIZE'].value_counts().index)
plt.title('Distribution of Deal Size - Count Plot')
plt.xlabel('Deal Size')
plt.ylabel('Count')
plt.show()
print('\n\n')

"""## This code visually represents the distribution of deal sizes through a pie chart and the distribution of average sales by product line using a bar plot in the DataFrame 'df.'"""

#Distribution of Deal Size - Pie Chart:
plt.figure(figsize = (8,8))
count_dealsize = df['DEALSIZE'].value_counts()
plt.pie(count_dealsize, labels = count_dealsize.index, autopct = '%1.1f%%', colors = sns.color_palette('colorblind'), wedgeprops = dict(width = 0.3))
plt.title('Distribution of Deal Size - Pie Chart')
plt.show()

#Distribution of Average Sales by Product line:
plt.figure(figsize = (10,8))
sns.barplot(x = 'PRODUCTLINE', y = 'SALES', data = df, palette = 'colorblind')
plt.title('Distribution of Average Sales by Product Line - Bar Plot')
plt.xlabel('Product Line')
plt.ylabel('Average Sales')
plt.show()
print('\n\n')

"""## This code calculates the correlation matrix for variables in the DataFrame 'df' and visualizes the relationships using a heatmap."""

#Finding relationships between variables:

correlation_matrix = df.corr()
print(correlation_matrix, '\n\n')
plt.figure(figsize = (8,8))
sns.heatmap(correlation_matrix, annot = True, cmap = 'coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

"""From the above heatmap we can see that QUANTITYORDERED vs SALES, PRICEEACH vs SALES, PRICEEACH vs MSRP and SALES vs MSRP are the strongest relationships.

## This code explores the relationship between 'QUANTITYORDERED' and 'SALES' through two joint plots: a scatter plot and a hexbin plot.
"""

#Relationship between Quantity Ordered and Sales:

#Joint Plot (Scatter) for Quantity Ordered VS Sales:
plt.figure(figsize=(8,8))
sns.jointplot(x = 'QUANTITYORDERED', y = 'SALES', data = df, kind = 'scatter')
plt.suptitle('Quantity Ordered VS Sales - Joint Plot', y = 1.02)
plt.show()
print('\n')

#Joint Plot for Quantity Ordered VS Sales:
plt.figure(figsize = (8,8))
sns.jointplot(x = 'QUANTITYORDERED', y = 'SALES', data = df, kind = 'hex', gridsize=20)
plt.suptitle('Quantity Ordered VS sales - Hexbin Plot', y = 1.02)
plt.show()
print('\n')

"""## This code fits a regression line to the scatter plot depicting the relationship between 'QUANTITYORDERED' and 'SALES' in the DataFrame 'df.'"""

#Fitting the regression line to the Scatter Plot:
plt.figure(figsize = (8,8))
sns.regplot(x = 'QUANTITYORDERED', y = 'SALES', data = df, scatter_kws={'s': 15, 'alpha': 0.5})
plt.title('Quantity Ordered VS Sales - Regression Plot')
plt.xlabel('Quantity Ordered')
plt.ylabel('Sales')
plt.show()

"""The output presents a scatter plot with a fitted regression line, illustrating the relationship between 'QUANTITYORDERED' and 'SALES' in the DataFrame 'df.' The regression line provides insights into the trend and direction of the association between the two variables.

## This code explores the relationship between 'PRICEEACH' and 'SALES' through two visualizations: a scatter plot and a hexbin plot.
"""

#Relationship between Price Each and Sales:

#Joint plot (Scatter) for Price Each VS Sales:
plt.figure(figsize=(8,8))
sns.jointplot(x = 'PRICEEACH', y = 'SALES', data = df, kind = 'scatter', color = 'green')
plt.suptitle('Price Each VS Sales - Joint Plot', y = 1.02)
plt.show()
print('\n')

#Joint plot (Hexbin) for Price Each VS Sales:
plt.figure(figsize = (8,8))
sns.jointplot(x = 'PRICEEACH', y = 'SALES', data = df, kind = 'hex', gridsize=20, color='green')
plt.suptitle('Price Each VS sales - Hexbin Plot', y = 1.02)
plt.show()
print('\n')

"""## This code fits a regression line to the scatter plot, depicting the relationship between 'Price Each' and 'Sales' in the DataFrame 'df.'"""

#Fitting the regression line to the Scatter Plot:
plt.figure(figsize = (8,8))
sns.regplot(x = 'PRICEEACH', y = 'SALES', data = df, color = 'green', scatter_kws={'s': 15, 'alpha': 0.5})
plt.title('Price Each VS Sales - Regression Plot')
plt.xlabel('Price Each')
plt.ylabel('Sales')
plt.show()

"""The output of the above code cell shows that regression line fits the scatter plot, giving insight on the behaviour and trend of the variables.

## This code explores the relationship between 'PRICEEACH' and 'MSRP' through two visualizations: a scatter plot and a hexbin plot.
"""

#Relationship between Price Each and MSRP:

#Joint Plot of Price Each VS MSRP:
plt.figure(figsize=(8,8))
sns.jointplot(x = 'PRICEEACH', y = 'MSRP', data = df, kind = 'scatter', color = 'red')
plt.suptitle('Price Each VS MSRP - Joint Plot', y = 1.02)
plt.show()
print('\n')

#Hexbin Plot of Price Each VS MSRP:
plt.figure(figsize = (8,8))
sns.jointplot(x = 'PRICEEACH', y = 'MSRP', data = df, kind = 'hex', gridsize=20, color='red')
plt.suptitle('Price Each VS MSRP - Hexbin Plot', y = 1.02)
plt.show()
print('\n')

"""## This code fits a regression line to the scatter plot, depicting the relationship between 'PRICEEACH' and 'MSRP' in the DataFrame 'df.'"""

#Fitting the Regression line to the Scatter Plot:
plt.figure(figsize = (8,8))
sns.regplot(x = 'PRICEEACH', y = 'MSRP', data = df, color = 'red', scatter_kws={'s': 15, 'alpha': 0.5})
plt.title('Price Each VS MSRP - Regression Plot')
plt.xlabel('Price Each')
plt.ylabel('MSRP')
plt.show()

"""The output showcases a scatter plot with a fitted regression line, illustrating the relationship between 'PRICEEACH' and 'MSRP' in the DataFrame 'df.' The regression line provides insights into the trend and direction of the association between the two variables.

## This code explores the relationship between 'SALES' and 'MSRP' through two visualizations: a scatter plot and a hexbin plot.
"""

#Relationship between Sales and MSRP:

#Joint Plot (Scatter) for Sales VS MSRP:
plt.figure(figsize=(8,8))
sns.jointplot(x = 'SALES', y = 'MSRP', data = df, kind = 'scatter', color = 'purple')
plt.suptitle('Sales VS MSRP - Joint Plot', y = 1.02)
plt.show()
print('\n')

#Joint Plot (Hexbin) for Sales VS MSRP:
plt.figure(figsize = (8,8))
sns.jointplot(x = 'SALES', y = 'MSRP', data = df, kind = 'hex', gridsize=20, color='purple')
plt.suptitle('Sales VS MSRP - Hexbin Plot', y = 1.02)
plt.show()
print('\n')

"""## This code fits a regression line to the scatter plot, depicting the relationship between 'SALES' and 'MSRP' in the DataFrame 'df.'"""

#Fitting the regression line to the Scatter Plot:
plt.figure(figsize = (8,8))
sns.regplot(x = 'SALES', y = 'MSRP', data = df, color = 'purple', scatter_kws={'s': 15, 'alpha': 0.5})
plt.title('Sales VS MSRP - Regression Plot')
plt.xlabel('Sales')
plt.ylabel('MSRP')
plt.show()

"""The output showcases a scatter plot with a fitted regression line, illustrating the relationship between 'SALES' and 'MSRP' in the DataFrame 'df.' The regression line provides insights into the trend and direction of the association between the two variables.

## This code explores the relationship between 'DEALSIZE' and 'SALES' through Box plot
"""

#Relationship between Deal Size and Sales:
print(df['DEALSIZE'].value_counts(), '\n\n')

#Box Plot for Deal Size VS Sales:
plt.figure(figsize =(8,8))
sns.boxplot(x= 'DEALSIZE', y = 'SALES', data = df, palette = 'colorblind')
plt.title('Deal Size VS Sales - Box Plot')
plt.xlabel('Deal Size')
plt.ylabel('Sales')
plt.show()
print('\n')

"""The output of this code block provides a summary of the distribution of sales across different deal sizes, offering insights into the variability and central tendency of sales within each deal size category. The box plot visually represents the quartiles, median, and potential outliers, aiding in the analysis of sales patterns based on deal sizes.

## Further exploration of the relationship between 'DEALSIZE' and 'SALES' through KDE plots and Histogram
"""

#Individual KDE plot for different Deal Size (Density VS Sales):
x1 = plt.figure(figsize=(8,8))
x1 = sns.FacetGrid(df, col = 'DEALSIZE', col_wrap = 3, height = 4)
x1.map(sns.kdeplot, 'SALES', fill = True, cmap = 'viridis')
x1.set_titles(col_template = "{col_name}")
x1.set_axis_labels('Sales', 'Density')
plt.show()
print('\n')

#Individual Histogram plot for different Deal Size (Density VS Sales):
plt.figure(figsize = (10,6))
sns.histplot(data = df, x = 'SALES', hue = 'DEALSIZE', multiple = 'stack', palette = 'colorblind', element = 'step', stat = 'density')
plt.title('Deal Size VS Sales')
plt.xlabel('Sales')
plt.ylabel('Density')
plt.show()

"""The output of this code block provides a detailed view of the distribution of sales density for different deal sizes. The individual KDE (Kernel Density Estimation) plots show the estimated probability density of sales values, while the stacked histogram plot provides a comparative visualization of sales density across various deal sizes. These visualizations help in understanding the variation in sales patterns within each deal size category and identifying potential trends or differences.

## Exploring the distribution of 'SALES' and 'PRODUCTLINE' on the basis of geography (Countries) using Count plot, Heatmap and KDE plot.
"""

#Geographical distribution of sales and productline:


#Table for each Product line count in various Countries:
product_line_counts = df.groupby(['COUNTRY', 'PRODUCTLINE']).size().reset_index(name='COUNT')
product_line_counts = product_line_counts.pivot(index='COUNTRY', columns='PRODUCTLINE', values='COUNT').fillna(0).astype(int)
print(product_line_counts, '\n\n')

#Count plot for Product line VS Country:
plt.figure(figsize=(10,10))
sns.countplot(x = 'COUNTRY', hue = 'PRODUCTLINE', data = df, palette = 'colorblind')
plt.title('Product Line VS Country - Count Plot')
plt.xlabel('Country')
plt.ylabel('Count')
plt.xticks(rotation=45, ha = 'right')
plt.legend(title = 'Product Line')
plt.show()
print('\n')

#Heatmap for Country VS Product line:
plt.figure(figsize=(8,8))
crosstab = pd.crosstab(df['COUNTRY'], df['PRODUCTLINE'])
sns.heatmap(crosstab, cmap = 'viridis', annot = True, fmt = 'd', cbar_kws = {'label': 'Count'})
plt.title('Country VS Product Line - Heatmap Plot')
plt.xlabel('Product Line')
plt.ylabel('Country')
plt.show()
print('\n')

#Distributions of Products based on Various Countries:
plt.figure(figsize=(8,8))
sns.kdeplot(x = 'SALES', data = df, hue = 'COUNTRY', palette = 'colorblind')
plt.title('Distribution of Sales in different Countries')
plt.show()
print('\n')

"""# Here, the recency, frequency and monetary value for each customer will be calculated for RFM analysis using relevant information [2]"""

# Convert 'ORDERDATE' to datetime format
df['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])

# Calculating Recency, Frequency, and Monetary Value
current_date = df['ORDERDATE'].max()

rfm_df = df.groupby('CUSTOMERNAME').agg({
    'ORDERDATE': lambda x: (current_date - x.max()).days,  # Recency
    'ORDERNUMBER': 'count',  # Frequency
    'SALES': 'sum'  # Monetary Value
}).reset_index()


rfm_df.columns = ['CUSTOMERNAME', 'Recency', 'Frequency', 'MonetaryValue']

# RFM DataFrame
print(rfm_df.head(50))

"""The rfm_df is created to calculate the rfm values and assign them to the respective customer.

# Once the RFM for all customers has been calculated, K will be determined using the Elbow method to use K-MEANS method later on
"""

# RFM features
rfm_features = rfm_df[['Recency', 'Frequency', 'MonetaryValue']]


scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_features)

# determining k using elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(rfm_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.show()

"""The output displays an elbow method plot with an L-shaped pattern, indicating distinct edges. The optimal cluster count (k) for subsequent KMeans clustering on standardized RFM features can be inferred from the point where the rate of within-cluster sum of squares reduction diminishes.

# Once k has been determined, K-MEANS method will be used to cluster our customers in different segments [2]
"""

#based on the Elbow method
num_clusters = 4

#k-means clustering
kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)
rfm_df['Cluster'] = kmeans.fit_predict(rfm_scaled)

#clustered RFM DataFrame
print(rfm_df.head(50))

"""The output of this cell shows the RFM DataFrame with an additional 'Cluster' column, indicating the assigned cluster for each observation based on the KMeans clustering with four clusters. The clustering results provide insights into grouping similar customer behaviors within the dataset.

# Here the clusters will be visualised using a scatter plot for better understanding
"""

#2D scatter plot
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=rfm_df, palette='viridis', s=100)


plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.title('2D Scatter Plot of Clusters')


plt.show()

"""The 2D scatter plot illustrates distinct clusters based on 'Recency' and 'Frequency,' with one cluster containing only two values, positioned above another cluster. The rest of the clusters are clearly defined in the graph, showcasing the clustering structure identified in the DataFrame 'rfm_df.'

# Here the RFM score will be evaluated by first standardizing the r, f, and m
"""

rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], q=5, labels=[5, 4, 3, 2, 1])
rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'], q=5, labels=[1, 2, 3, 4, 5])
rfm_df['M_Score'] = pd.qcut(rfm_df['MonetaryValue'], q=5, labels=[1, 2, 3, 4, 5])

# Combine the scores to create the RFM Score
rfm_df['RFM_Score'] = rfm_df['R_Score'].astype(str) + rfm_df['F_Score'].astype(str) + rfm_df['M_Score'].astype(str)
print(rfm_df.head(80))

"""The output showcases the RFM DataFrame augmented with individual scores for Recency ('R_Score'), Frequency ('F_Score'), MonetaryValue ('M_Score'), and a combined RFM Score ('RFM_Score'). These scores categorize customers based on their transaction recency, frequency, and monetary value, facilitating further segmentation and analysis.

# To further apply predictive models to our data using the RFM score and other relevant features, some features will be dropped to increase the overall computation of our model.
"""

columns_to_exclude = ['CUSTOMERNAME']

# Extract the 'CUSTOMERNAME' column and drop all other object-type columns
df1 = df[columns_to_exclude + df.select_dtypes(exclude=['object']).columns.tolist()]
df1.drop('ORDERDATE', axis=1, inplace=True)
print(df1.info())

"""# The dataframe has been excluded of all data types other than int and float, the segmenting data and sales data will be merged for modelling."""

merged_df = pd.merge(df1, rfm_df, on='CUSTOMERNAME', how='inner')
merged_df.info()

merged_df.head()

"""# Once our data frame is ready for modelling, the data will be split in test and train data followed by applying various predicitive models"""

# Extract features and target variable
X = merged_df.drop(['PRICEEACH', 'CUSTOMERNAME', 'Cluster', 'RFM_Score','MSRP'], axis=1)  # Features
y = merged_df['PRICEEACH']  # Target variable

# Convert categorical RFM scores to numeric values
X['R_Score'] = X['R_Score'].astype(int)
X['F_Score'] = X['F_Score'].astype(int)
X['M_Score'] = X['M_Score'].astype(float)  # Convert to float since it's originally a category

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#function definition for evaluating the models based on MSE and R-SQUARED values

def evaluate_model(predictions, true_values, model_type):
    if model_type == 'regression':
        mse = mean_squared_error(true_values, predictions)
        r2 = r2_score(true_values, predictions)
        return mse, r2

    else:
        print("Invalid model type specified.")


# Train and test models for the entire dataset
# Linear Regression
linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train, y_train)
linear_reg_predictions = linear_reg_model.predict(X_test)

# Evaluating the linear Regression model
model = "regression"
print("\nLinear Regression:")
linear_mse, linear_r2 = evaluate_model(linear_reg_predictions, y_test, model)
print('Mean Squared Error: ',linear_mse)
print('R-squared (R2): ',linear_r2, '\n')

#Decision Tree Regressor
decision_tree_model = DecisionTreeRegressor(random_state=42)
decision_tree_model.fit(X_train, y_train)
decision_tree_predictions = decision_tree_model.predict(X_test)

# Evaluating the Decision Tree model
model = "regression"
print("\nDecision Tree Regressor:")
decision_mse, decision_r2 = evaluate_model(decision_tree_predictions, y_test, model)
print('Mean Squared Error: ',decision_mse)
print('R-squared (R2): ',decision_r2, '\n')

# Random Forest Regressor
random_forest_model = RandomForestRegressor(random_state=42)
random_forest_model.fit(X_train, y_train)
random_forest_predictions = random_forest_model.predict(X_test)

# Evaluate the Random Forest model
model = "regression"
print("\nRandom Forest Regressor:")
randomf_mse, randomf_r2 = evaluate_model(random_forest_predictions, y_test, model)
print('Mean Squared Error: ',randomf_mse)
print('R-squared (R2): ',randomf_r2, '\n')

#Lasso Regression
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train, y_train)
lasso_predictions = lasso_model.predict(X_test)

# Evaluating the Lasso Regression model
model = "regression"
print("\nLasso Regression:")
lasso_mse, lasso_r2 = evaluate_model(lasso_predictions, y_test, model)
print('Mean Squared Error: ',lasso_mse)
print('R-squared (R2): ',lasso_r2, '\n')

#Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
ridge_predictions = ridge_model.predict(X_test)

# Evaluating the Ridge Regression model
model = "regression"
print("\nRidge Regression:")
ridge_mse, ridge_r2 = evaluate_model(ridge_predictions, y_test, model)
print('Mean Squared Error: ',ridge_mse)
print('R-squared (R2): ',ridge_r2, '\n')

#Bayesian Ridge Regression
bayesian_ridge_model = BayesianRidge()
bayesian_ridge_model.fit(X_train, y_train)
bayesian_ridge_predictions = bayesian_ridge_model.predict(X_test)

# Evaluating the Bayesian Ridge Regression model
model = "regression"
print("\nBayesian Ridge Regression:")
bayesian_mse, bayesian_r2 = evaluate_model(bayesian_ridge_predictions, y_test, model)
print('Mean Squared Error: ',bayesian_mse)
print('R-squared (R2): ',bayesian_r2, '\n')

mse_values = [['Linear Regression', linear_mse],['Decision Tree Regressor',decision_mse], ['Random Forest Regressor',randomf_mse],['Lasso Regression',lasso_mse],['Ridge Regression',ridge_mse], ['Bayesian Ridge Regression',bayesian_mse]]
r2_values = [['Linear Regression', linear_r2],['Decision Tree Regressor',decision_r2], ['Random Forest Regressor',randomf_r2],['Lasso Regression',lasso_r2],['Ridge Regression',ridge_r2], ['Bayesian Ridge Regression',bayesian_r2]]

"""The code includes feature extraction, dataset splitting, standardization, and evaluation of various regression models.

Model performance is assessed using Mean Squared Error (MSE) and R-squared values and from the above output we can observe that amonst all the models, Random Forest Regressor performed the best.

## Visualisation of the performances of the models
"""

# Extracting data for plotting
regression_name, values = zip(*mse_values)

# Creating the bar graph
plt.figure(figsize=(10, 6))
plt.bar(regression_name, values, color='blue')
plt.xlabel('Regression Models')
plt.ylabel('Mean squared Error (MSE)')
plt.title('Bar Graph of MSE values for different models')
plt.xticks(rotation=45, ha='right')
plt.show()
print('\n\n')

# Extracting data for plotting
regression_name, values = zip(*r2_values)

# Creating the bar graph
plt.figure(figsize=(10, 6))
plt.bar(regression_name, values, color='green')
plt.xlabel('Regression Models')
plt.ylabel('R-squared (R2)')
plt.ylim(0.88, 1.0)
plt.title('Bar Graph of R2 values for different models')
plt.xticks(rotation=45, ha='right')
plt.show()

"""From the above graphs we can see that Random Forest regressor has the highest R2 score and lowest mean squared error.

# Once alll models have been made, the predicted values vs true values will be visualised for better understanding
"""

# Function to plot predicted vs true values
def plot_predictions(true_values, predicted_values, model_name):
    sns.regplot(x = true_values, y = predicted_values,scatter_kws={'s': 15, 'alpha': 0.5}, line_kws={'color': 'red'} )
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{model_name} - True vs Predicted Values')
    plt.show()
    print('\n')


# Plot Linear Regression predictions
plot_predictions(y_test, linear_reg_predictions, 'Linear Regression')

# Plot Decision Tree predictions
plot_predictions(y_test, decision_tree_predictions, 'Decision Tree Regressor')

# Plot Random Forest predictions
plot_predictions(y_test, random_forest_predictions, 'Random Forest Regressor')

# Plot Lasso Regression predictions
plot_predictions(y_test, lasso_predictions, 'Lasso Regression')

# Plot Ridge Regression predictions
plot_predictions(y_test, ridge_predictions, 'Ridge Regression')

# Plot Bayesian Ridge Regression predictions
plot_predictions(y_test, bayesian_ridge_predictions, 'Bayesian Ridge Regression')

"""The output shows the regression plots (true values vs predicted values) for the different models that we used and fits a regression line to them. After looking at the output we can see that Random Forest regressor and Decision Tree regressor are the best performers, and out of these two, Random Forest regressor performs the best.

# Project Outcome

## Overview of Results
Comprehensive analysis of the automotive sales dataset has provided valuable insights to optimize business strategies in the automotive industry.
At first after having a brief overview about the data, exploratory data analysis (EDA) identified important patterns and trends in customer buying behavior by analsying factors such as Product line, deal size, Sales etc. A deeper understanding of the data was driven by analysing various factors according to geographical distribution as well.

By using clustering algorithms to segment customers based on recency, frequency, and monetary value, it is now possible to develop targeted marketing strategies tailored to different customer profiles based on the RFM score of customer segments.

Developing and evaluating order price prediction models that incorporate variables such as MSRP and RFM score provided a means to improve pricing strategies according to the segments created.
Visualizations created with Matplotlib and Seaborn effectively conveyed the accuracy of different models and helped us choose the most reliable predictive model.
Despite the potential challenges and limitations faced during  analysis, this project supports companies with actionable recommendations derived from a combination of EDA, customer segmentation, predictive modeling, and impactful visualizations.

These insights not only contribute to a deeper understanding of customer preferences, but also provide the basis for informed decision-making, ultimately increasing customer satisfaction and competitiveness in a dynamic automotive market.

## Exploratory Analysis of Purchase Patterns

### Explanation of Results

Performing Exploratory Data Analysis using various techniques on our dataset, we have identified the following patterns in vehicle sales:

1) As observed in the Sales distribution graph, the maximum number of orders is having Sales between 1800 and 4000 with average Sales of the manufacturer being 3553.

2) The highest selling product line of the manufacturer being 'Classic Cars' with 949 units sold, followed by 'Vintage Cars' and 'Motorcycles' which has significantly lower Sales figure as compared to 'Classic Cars' while the least sold Product Line being 'Train' which accounts for less than 3% of the total Sales.

3) USA is the biggest market for the manufacturer accounting for over one third of the net orders, followed by Spain(12.4%) and France(11.4%).

4) An interesting observation which can be inferred from the country wise product line count plot is that the top two product lines for USA and Spain are 'Classic Cars' and 'Vintage Cars' while that for France are 'Classic Cars' and 'Motorcycles'.

5) Deal size for the majority of orders is 'Medium' and 'Small' while 'Large' contributes less than 6% of the manufacturer's net sales.

6) The visualisation of the relationship between price of each product, quantity ordered and sales depict a linear relationship which infers the higher the order quantity or the higher the price of each product, the higher will be the total sales value.

7) Quantity ordered is majorly concentrated between 20 and 50 with few outliers reaching above 70.

_Note: 'DEALSIZE' is a categorised version of 'SALES' where if,
   0 < 'SALES' < 3000 => 'DEALSIZE' = 'Small';
3000 < 'SALES' < 5000 => 'DEALSIZE' = 'Medium';
       'SALES' > 5000 => 'DEALSIZE' = 'Large'._
       

### Visualisation
_The following visualisations depict all the above finding._
"""

#Distribution of Sales:
plt.figure(figsize = (8,8))
sns.histplot(df['SALES'], kde = True, color = 'red', stat = 'density')
plt.title('Distribution of Sales - Histogram')
plt.xlabel('Sales')
plt.ylabel('Density')
plt.show()
print('\n\n')

#Distribution of Product line:
plt.figure(figsize = (10,8))
sns.countplot(y = 'PRODUCTLINE', data = df, palette = 'colorblind', order = df['PRODUCTLINE'].value_counts().index)
plt.title('Distribution of Average Sales by Product Line - Bar Plot')
plt.xlabel('Total Units Sold')
plt.ylabel('Product Line')
plt.show()
print('\n\n')

#Distribution of Country:
plt.figure(figsize = (10,8))
country_counts = df['COUNTRY'].value_counts()
plt.pie(country_counts, labels = country_counts.index, autopct = '%1.1f%%', colors = sns.color_palette('colorblind'))
plt.title('Distribution of Country - Pie Chart')
plt.show()
print('\n\n')

#Count plot for Product line VS Country:
plt.figure(figsize=(10,10))
sns.countplot(x = 'COUNTRY', hue = 'PRODUCTLINE', data = df, palette = 'colorblind')
plt.title('Product Line VS Country - Count Plot')
plt.xlabel('Country')
plt.ylabel('Count')
plt.xticks(rotation=45, ha = 'right')
plt.legend(title = 'Product Line')
plt.show()
print('\n')


#Distribution of Deal Size:
plt.figure(figsize = (8,8))
sns.countplot(x = 'DEALSIZE', data = df, palette = 'colorblind', order = df['DEALSIZE'].value_counts().index)
plt.title('Distribution of Deal Size - Count Plot')
plt.xlabel('Deal Size')
plt.ylabel('Count')
plt.show()
print('\n\n')

#Joint plot (Scatter) for Price Each VS Sales:
plt.figure(figsize=(8,8))
sns.jointplot(x = 'PRICEEACH', y = 'SALES', data = df, kind = 'scatter', color = 'green')
plt.suptitle('Price Each VS Sales - Joint Plot', y = 1.02)
plt.show()
print('\n')

#Fitting the regression line to the Scatter Plot:
plt.figure(figsize = (8,8))
sns.regplot(x = 'QUANTITYORDERED', y = 'SALES', data = df, scatter_kws={'s': 15, 'alpha': 0.5})
plt.title('Quantity Ordered VS Sales - Regression Plot')
plt.xlabel('Quantity Ordered')
plt.ylabel('Sales')
plt.show()

"""## Customer Segmentation using Clustering
### Explanation of Results

RFM is a technique that segments customer by their purchasing pattern [3]. We are segregating the customers into groups using RFM to assign the existing customers a loyalty rating and also to use the RFM score for predictive modelling later in this project.

In this section we have classified each unique customer based on three different factors:

1) Recency(time since last order): The recency value for each customer is calculated as the number of days between the most recent purchase date of the customer and the overall latest date in the dataset.

2) Frequency(number of orders): The frequency value is calculated by counting unique order numbers of each customer, representing the frequency of orders made by that particular customer.

3) Monetary(total sales): The monetary score is calculated by just summing up the total sales from each customer.

Once we get the RFM features of each customer we apply standardisation to transform the data to have a mean of 0 and a standard deviation of 1 as it ensures that all features contribute equally to the distance computation when k-means algorithm is applied.

Then we try to figure out the optimum number of customer cluster for the scaled RFM data by plotting the sum of square(distance) within cluster versus number of cluster and as observed from the elbow method plot, the entire customer list can be segmented into 4 clusters based on their loyalty score.

### Visualisation
_The 2-D scatter plot below perfectly depicts the customer clusters._
"""

#2D scatter plot
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=rfm_df, palette='viridis', s=100)

plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.title('2D Scatter Plot of Clusters')

plt.show()

"""## Predictive Modeling for Price Determination
### Explanation of Results

In this section we merged the coustomer segmentation dataset with the main one on customer name to assign previously calculated RFM scores to each customer as we are going to use the score as one of our input feature to train various models. Once we get the complete dataset we start looking for features which has significant(not too high) correlation with the feature we are trying to predict('PRICEEACH'). Observing the coorelation matrix, we conclude to drop the following features from the final dataset:

1) 'PRICEEACH': As we are trying to predict this feature it becomes the output of the model hence droped.

2) 'CUSTOMERNAME': It does not have any coorelation with 'PRICEEACH'.

3) 'Cluster': This is a categorical value of RFM scores which we have already taken as input feature.

4) 'RFM_Score': This is an aggregate score of R_score, M_score and F_score which we have already taken as input feature.

5) 'MSRP': It has high correlation with 'PRICEEACH', so to avoid redundancy we drop this feature.

Then we split the entire dataset into train-test set of 4:1 ratio and standardised both the sets before feeding them to various models and evaluating accuracies of those models.

### Visualisation
_The below heat map vividly illustrates the correlation between different features in the dataset._
"""

#Finding relationships between variables:
correlation_matrix = df.corr()
print(correlation_matrix, '\n\n')
plt.figure(figsize = (8,8))
sns.heatmap(correlation_matrix, annot = True, cmap = 'coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

"""## Visualization of Model Accuracy
### Explanation of Results

Finally, after training various models on the same set of features and testing on the remaining dataset, we have quite a few interesting observations:

1) Linear Regression:

Mean Squared Error: 122.87
R-squared (R2): 0.93
Linear Regression seems to provide a good fit with an R2 of 0.93, indicating that about 93% of the variance in the data is captured by the model. The Mean Squared Error is decent, though not the lowest.

2) Decision Tree Regressor:

Mean Squared Error: 50.63
R-squared (R2): 0.97
Decision Tree Regressor shows improved performance compared to Linear Regression, with a lower Mean Squared Error and a higher R2 value. It seems to capture the underlying patterns in the data more effectively.

3) Random Forest Regressor:

Mean Squared Error: 14.12
R-squared (R2): 0.99
Random Forest Regressor stands out with the lowest Mean Squared Error and a near-perfect R2 value of 0.99. This suggests that it's doing an excellent job in predicting the target variable, likely due to its ensemble nature.

4) Lasso Regression:

Mean Squared Error: 120.96
R-squared (R2): 0.93
Lasso Regression performs similarly to Linear Regression, but with a slightly higher Mean Squared Error. The R2 value is still in the same ballpark, indicating good explanatory power.

5) Ridge Regression:

Mean Squared Error: 122.52
R-squared (R2): 0.93
Ridge Regression results are close to Linear Regression, suggesting that regularization might not be significantly impacting the model's performance in this case.

6) Bayesian Ridge Regression:

Mean Squared Error: 122.68
R-squared (R2): 0.93
Similar to Ridge Regression, Bayesian Ridge Regression provides results comparable to Linear Regression, indicating stability but not a substantial improvement.

Among the models, Random Forest Regressor appears to be the winner, delivering the lowest Mean Squared Error and the highest R-squared value.

### Visualisation
_The below bar graph along with the regression plot clearly proves Random Forest Regression model is the best performer among all the models._
"""

# Creating the bar graph
plt.figure(figsize=(10, 6))
plt.bar(regression_name, values, color='blue')
plt.xlabel('Regression Models')
plt.ylabel('Mean squared Error (MSE)')
plt.title('Bar Graph of MSE values for different models')
plt.xticks(rotation=45, ha='right')
plt.show()
print('\n\n')

# Creating the bar graph
plt.figure(figsize=(10, 6))
plt.bar(regression_name, values, color='green')
plt.xlabel('Regression Models')
plt.ylabel('R-squared (R2)')
plt.ylim(0.88, 1.0)
plt.title('Bar Graph of R2 values for different models')
plt.xticks(rotation=45, ha='right')
plt.show()

# Plot Random Forest predictions
plot_predictions(y_test, random_forest_predictions, 'Random Forest Regressor')

"""# Conclusion (5 marks)

Overall our project can be concluded as follows:

### Achievements
In this project we have thoroughly analysed the sales data of an automotive manufacturer and uncovered key facts and figures about their customer's behaviour and demographics which will assist the business in understanding, serving and retaining their customers better.

We have also successfully segregated the customer base into multiple groups based on their value and loyalty which will eventually help the business to design loyalty schemes to retain high value customers and improve new customer acquisition strategy better.

Finally we noticed that price each of different products are being assigned dynamic values, hence we scrutinized relationship between price of each product with various factors like quantity ordered, MSRP and RMF scores of the customer to train a machine learning model to predict the price each for every order. This model will help automate the dynamic pricing of each product and hence will optimize the the Order To Cash process of the business. The accuracy of the model is very close to that of the original data.

### Limitations

This project has the following limitations:

1) The dataset used has no missing values, neither does it have any inconsistencies hence it does not represent real world data where there might be many inconsistencies giving rise to complications.

2) The dataset also has very little outliers which can be a problem for the predictive when used in the real world scenario.

2) The model and the analysis is not generalised, hence is valid only for this automobile manufacturer and their dataset.

### Future Work

In future work we would like to acquire high volume of data corresponding more to real world sales of the business to train the model for improved real world performance. We would also like to drop or combine few features used in the model to generalise it's application to make it industry independent. For example: to predict the sales of ice cream or clothes and not just vehicles.

# References

[1] dee dee(https://www.kaggle.com/ddosad). 2023. Automobile Sales data. Kaggle. [Online]. [Accessed 22 November 2023]. Available from: https://www.kaggle.com/datasets/ddosad/auto-sales-data/data .

[2] Kabasakal, I. 2020. Customer Segmentation Based On Recency Frequency Monetary Model: A Case Study in E-Retailing. BiliÅŸim Teknolojileri Dergisi. 13 (1), pp.48-55.

[3] Murphy, C. 2022. What Is Recency, Frequency, Monetary Value (RFM) in Marketing?. [Online]. [Accessed 22 November 2023]. Available from: https://www.investopedia.com/
"""